# FinSolve ABAC RAG (Simple Guide)

This project is an internal chatbot for a company (FinSolve). It answers questions using your documents and follows strict access rules. We use an ABAC policy (Attribute Based Access Control) and a RAG pipeline (Retrieval Augmented Generation).

If you just want to run and try, follow Quick Start below. 

---

## What it can do
- Secure login (basic, for demo)
- Finds best information using:
  - Dense search (Chroma + OpenAI embeddings)
  - Sparse search (BM25)
  - Fusion of both (RRF)
  - Optional reranking (simple | OpenAI embedding | OpenAI LLM)
- Policy checks (PDP) on every item, so people only see what they are allowed
- Answers are generated by LLM only from allowed context and always have citations
- HR data endpoints with masking and aggregates (policy controlled)
- Observability: timings and a correlation ID per request
- Admin panel (for c_level) to check status and reindex

---

## Quick Start
Requirements:
- Python 3.10+
- pip
- (Optional) virtualenv
- OpenAI API key if you want dense index and live LLM responses

Steps:
1) Clone and go inside folder
```
 git clone <your repo url>
 cd finsolve-abac-rag
```
2) Create and activate virtual env (optional but recommended)
```
 python -m venv .venv
 source .venv/bin/activate   # Windows: .venv\Scripts\activate
```
3) Install packages
```
 pip install -e .
```
4) Set OpenAI key (for embeddings + LLM)
```
 export OPENAI_API_KEY=sk-...
```
You can also keep it in a `.env` file at project root like:
```
 OPENAI_API_KEY=sk-...
```
5) Run app
```
 uvicorn app.main:app --reload
```
6) Open your browser
- Go to: http://127.0.0.1:8000/
- Login when prompted (see Users below)

---

## Users and roles (demo)
- Tony / password123 → engineering
- Bruce / securepass → marketing
- Sam / financepass → finance
- Peter / pete123 → engineering
- Sid / sidpass123 → marketing
- Natasha / hrpass123 → hr
- Clark / chief → c_level (admin panel visible)

These are only for demo.

---

## Using the app
- Type your question on the page and submit
- You can change Top K and select reranker
- You will get answer, citations, timings, and a correlation ID
- If you login as Clark (c_level), you can see Admin panel buttons:
  - Refresh Status: shows counts (sparse/dense) and OpenAI health
  - Reindex (sparse): build in-memory BM25 index
  - Reindex (dense): build Chroma index (needs OpenAI key)

---

## Endpoints (short)
- UI: GET `/`
- Chat: POST `/chat`
- Search: POST `/search/dense`, POST `/search/hybrid`
- HR: GET `/hr/rows`, `/hr/aggregate`, `/hr/rows_masked`
- Admin: POST `/admin/reindex`, POST `/admin/reindex_dense`, GET `/admin/status`

All endpoints require login. Some need specific roles (policy enforced).

---

## Configuration
- `OPENAI_API_KEY` → set this for embeddings + LLM
- `OPENAI_EMBEDDING_MODEL` (default: text-embedding-3-small)
- `OPENAI_CHAT_MODEL` (default: gpt-4o-mini)
- `CHROMA_DB_DIR` (default: .chroma)
- `HR_AGGREGATE_MODE` / `HR_MASKED_ROWS_MODE` → enable/disable features

You can put these in `.env`.

---

## Architecture & Policy
- Full document with diagram: `docs/ARCHITECTURE.md`
- Policy file: `docs/policy.yaml`

In short: We do hybrid retrieval, check PDP policy for every item, and then only pass allowed snippets to LLM. The LLM must answer only from given context and must cite sources.

---

## Testing
Run test suite:
```
 pytest -q
```
Tests use monkeypatching for external calls, so they are stable and do not need real network.

---


---

## E2E Retrieval Test Results (Live with my OpenAI key)

I ran a few end-to-end checks locally using real embeddings and hybrid search.

- How I tested
  - Endpoint: POST /search/hybrid (dense + sparse + policy)
  - I set OPENAI_API_KEY in env, so dense search was active
  - top_k = 3, no reranking (kept simple)

- What I saw
  1) Query: "marketing ROI 2024" as Bruce (marketing)
     - Sources came from marketing reports
     - Example: resources/data/marketing/market_report_q4_2024.md (Q4 overview)
     - Policy: allowed
  2) Query: "Salary Structure" as Bruce (marketing)
     - Source: resources/data/general/employee_handbook.md (Salary Structure section)
     - Policy: allowed
  3) Query: "Q4 2024 revenue 2.6 billion" as Sam (finance)
     - Source: resources/data/finance/quarterly_financial_report.md (Q4 overview)
     - Policy: allowed
  4) Query: "employee salary details" as Bruce (marketing)
     - Only general handbook sections returned (no HR CSV or HR-only docs)
     - Policy blocked HR-only content for non-HR roles

- Speed (rough): dense_ms about 0.9–1.5s per query

- My conclusion
  - It is pulling the right information from the right documents
  - Policy is working correctly for roles


## Chat results I saw (Live with my OpenAI key)

I also tried the chat endpoint end-to-end. These are short notes of what I saw.

- marketing ROI 2024 (as Bruce/marketing)
  - Answer talked about Q1 target ~3x ROI with ~$2M spend and Q4 focus with ~$2.5M spend, aiming to maximise ROI.
  - Citations:
    - resources/data/marketing/market_report_q4_2024.md (Q4 overview, conclusion)
    - resources/data/marketing/marketing_report_q1_2024.md (Projections & Targets)

- employee salary details (as Bruce/marketing)
  - Answer listed salary components (Basic, HRA, Special Allowance, Conveyance, Bonus, Gratuity, PF, etc.).
  - Citations:
    - resources/data/general/employee_handbook.md (Salary Structure, Statutory Benefits, Payroll Schedule)

- Q4 2024 revenue 2.6 billion (as Sam/finance)
  - Answer said: “I don't know based on the available context.” (model was cautious)
  - Citations still pointed to correct finance report sections:
    - resources/data/finance/quarterly_financial_report.md (Q4 overview, Executive Summary, Q2 overview)

Note: Retrieval and policy are correct. For the finance question, the model was conservative even with the right context. If needed, we can relax the generation prompt slightly so it extracts clear figures from cited context.

## Common issues
- If chat works but quality is low: maybe dense index is empty. Login as `Clark/chief` and click “Reindex (dense)” in Admin panel (need `OPENAI_API_KEY`).
- If embeddings or LLM calls fail: check `OPENAI_API_KEY` and internet access.

---

