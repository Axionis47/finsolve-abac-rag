# Copy this file to .env and fill in your values
# NEVER commit your actual .env file!

# =============================================================================
# Backend Selection
# =============================================================================
# LLM Backend: "openai" | "vllm" | "ollama" | "vertex"
LLM_BACKEND=vertex

# Embedding Backend: "openai" | "local" | "tei" | "vertex"
EMBEDDING_BACKEND=vertex

# =============================================================================
# Google Cloud / Vertex AI Configuration (RECOMMENDED for production)
# =============================================================================
# GCP Project ID (required for vertex backend)
GCP_PROJECT=your-gcp-project-id
VERTEX_PROJECT=your-gcp-project-id

# GCP Region (default: us-central1)
VERTEX_LOCATION=us-central1

# Vertex AI LLM Model (Gemini models)
# Options: gemini-1.5-flash, gemini-1.5-pro, gemini-2.0-flash-exp
VERTEX_MODEL=gemini-1.5-flash

# Vertex AI Embedding Model
# Options: text-embedding-005, text-embedding-004, text-multilingual-embedding-002
VERTEX_EMBEDDING_MODEL=text-embedding-005

# Authentication: Set GOOGLE_APPLICATION_CREDENTIALS to service account key path
# Or use: gcloud auth application-default login
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json

# =============================================================================
# OpenAI Configuration (when using openai backend)
# =============================================================================
OPENAI_API_KEY=sk-your-key-here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_CHAT_MODEL=gpt-4o-mini

# =============================================================================
# vLLM Configuration (when using vllm backend)
# =============================================================================
VLLM_BASE_URL=http://localhost:8000/v1
VLLM_MODEL=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4

# =============================================================================
# Ollama Configuration (when using ollama backend)
# =============================================================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# =============================================================================
# Local Embedding Configuration (when using local backend)
# =============================================================================
LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2

# =============================================================================
# TEI Configuration (when using tei backend)
# =============================================================================
TEI_BASE_URL=http://localhost:8080
TEI_MODEL=BAAI/bge-small-en-v1.5

# =============================================================================
# Application Configuration
# =============================================================================
CHROMA_DB_DIR=.chroma
LLM_CACHE_DIR=.llm_cache
LLM_CACHE_ENABLED=true
HR_AGGREGATE_MODE=disabled
HR_MASKED_ROWS_MODE=disabled
HOST=127.0.0.1
PORT=8000
